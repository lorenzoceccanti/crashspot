{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6841f938",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Data Mining and Machine Learning </h2>\n",
    "<h3 align=\"center\"> Final Project </h3>\n",
    "<h2 align=\"center\"> <b> <i> CrashSpot </i> </b> </h2>\n",
    "<h4 align=\"center\"> Lorenzo Ceccanti matr. 564490 </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9e5c5",
   "metadata": {},
   "source": [
    "### <b> Data Integration </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada2139",
   "metadata": {},
   "source": [
    "We begin this phase from the cleaned version of the dataset in which the granularity is for accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d891239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inverse_data</th>\n",
       "      <th>week_day</th>\n",
       "      <th>hour</th>\n",
       "      <th>state</th>\n",
       "      <th>road_id</th>\n",
       "      <th>km</th>\n",
       "      <th>city</th>\n",
       "      <th>cause_of_accident</th>\n",
       "      <th>type_of_accident</th>\n",
       "      <th>victims_condition</th>\n",
       "      <th>weather_timestamp</th>\n",
       "      <th>road_direction</th>\n",
       "      <th>wheather_condition</th>\n",
       "      <th>road_type</th>\n",
       "      <th>road_delineation</th>\n",
       "      <th>people</th>\n",
       "      <th>deaths</th>\n",
       "      <th>slightly_injured</th>\n",
       "      <th>severely_injured</th>\n",
       "      <th>uninjured</th>\n",
       "      <th>unharmed</th>\n",
       "      <th>total_injured</th>\n",
       "      <th>vehicles_involved</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>regional</th>\n",
       "      <th>police_station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>sunday</td>\n",
       "      <td>01:45:00</td>\n",
       "      <td>RS</td>\n",
       "      <td>116.0</td>\n",
       "      <td>34,9</td>\n",
       "      <td>VACARIA</td>\n",
       "      <td>Mechanical loss/defect of vehicle</td>\n",
       "      <td>Rear-end collision</td>\n",
       "      <td>With injured victims</td>\n",
       "      <td>Night</td>\n",
       "      <td>Decreasing</td>\n",
       "      <td>Clear sky</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Straight</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.50712</td>\n",
       "      <td>-50.94118</td>\n",
       "      <td>SPRF-RS</td>\n",
       "      <td>DEL05-RS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>sunday</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>PR</td>\n",
       "      <td>376.0</td>\n",
       "      <td>636</td>\n",
       "      <td>TIJUCAS DO SUL</td>\n",
       "      <td>Incompatible velocity</td>\n",
       "      <td>Run-off-road</td>\n",
       "      <td>With dead victims</td>\n",
       "      <td>Night</td>\n",
       "      <td>Increasing</td>\n",
       "      <td>Drizzle</td>\n",
       "      <td>Double</td>\n",
       "      <td>Curve</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-25.75400</td>\n",
       "      <td>-49.12660</td>\n",
       "      <td>SPRF-PR</td>\n",
       "      <td>DEL01-PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>sunday</td>\n",
       "      <td>04:40:00</td>\n",
       "      <td>BA</td>\n",
       "      <td>101.0</td>\n",
       "      <td>65</td>\n",
       "      <td>ENTRE RIOS</td>\n",
       "      <td>Driver was sleeping</td>\n",
       "      <td>Head-on collision</td>\n",
       "      <td>With dead victims</td>\n",
       "      <td>Sunrise</td>\n",
       "      <td>Decreasing</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Curve</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-11.96180</td>\n",
       "      <td>-38.09530</td>\n",
       "      <td>SPRF-BA</td>\n",
       "      <td>DEL01-BA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  inverse_data week_day      hour state  road_id    km            city  \\\n",
       "0   2017-01-01   sunday  01:45:00    RS    116.0  34,9         VACARIA   \n",
       "1   2017-01-01   sunday  01:00:00    PR    376.0   636  TIJUCAS DO SUL   \n",
       "2   2017-01-01   sunday  04:40:00    BA    101.0    65      ENTRE RIOS   \n",
       "\n",
       "                   cause_of_accident    type_of_accident  \\\n",
       "0  Mechanical loss/defect of vehicle  Rear-end collision   \n",
       "1              Incompatible velocity        Run-off-road   \n",
       "2                Driver was sleeping   Head-on collision   \n",
       "\n",
       "      victims_condition weather_timestamp road_direction wheather_condition  \\\n",
       "0  With injured victims             Night     Decreasing          Clear sky   \n",
       "1     With dead victims             Night     Increasing            Drizzle   \n",
       "2     With dead victims           Sunrise     Decreasing             Cloudy   \n",
       "\n",
       "  road_type road_delineation  people  deaths  slightly_injured  \\\n",
       "0    Simple         Straight       6       0                 4   \n",
       "1    Double            Curve       2       1                 0   \n",
       "2    Simple            Curve       5       1                 1   \n",
       "\n",
       "   severely_injured  uninjured  unharmed  total_injured  vehicles_involved  \\\n",
       "0                 0          2         0              4                  2   \n",
       "1                 0          1         0              0                  2   \n",
       "2                 1          2         0              2                  2   \n",
       "\n",
       "   latitude  longitude regional police_station  \n",
       "0 -28.50712  -50.94118  SPRF-RS       DEL05-RS  \n",
       "1 -25.75400  -49.12660  SPRF-PR       DEL01-PR  \n",
       "2 -11.96180  -38.09530  SPRF-BA       DEL01-BA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(os.path.join('editedDataset', 'CLEANED_brasilEnglishAggr.csv'))\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88175036",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce40c6c",
   "metadata": {},
   "source": [
    "# Controllare le coordinate dal primissimo file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00ed6e",
   "metadata": {},
   "source": [
    "Let's inspect the dataset in which the granularity is per occupant.\n",
    "\n",
    "This dataset is not in English, so first it requires a little bit of translation\n",
    "\n",
    "The source of data however it's the same. Each file contains different years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857f3db",
   "metadata": {},
   "source": [
    "<b> Problem </b>: If I try to import directly the dataset `BRASIL_RAW` we obtain an UnicodeDecodeError. We discover that the encoding for the dataset is not UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ddf05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding for acidentes2017: ISO-8859-1\n",
      "Encoding for acidentes2018: ISO-8859-1\n",
      "Encoding for acidentes2019: ISO-8859-1\n",
      "Encoding for acidentes2020: ISO-8859-1\n",
      "Encoding for acidentes2021: ISO-8859-1\n",
      "Encoding for acidentes2022: ISO-8859-1\n",
      "Encoding for acidentes2023: ISO-8859-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chardet\n",
    "\n",
    "for i in range(2017,2024):\n",
    "    with open(os.path.join('dataset/BRASIL_RAW/acidentes', f'acidentes{i}.csv'), 'rb') as f:\n",
    "        result = chardet.detect(f.read(10000))  # leggi i primi 10k byte\n",
    "        print(f'Encoding for acidentes{i}: ' + result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb55cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This little script converts the original brasil_raw into UTF-8 encoding\n",
    "\n",
    "# Checking if the new directory we want to create already exists\n",
    "out_dir = \"editedDataset/UTF_acidentes\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "    for i in range(2017,2024):\n",
    "        src = os.path.join('dataset/BRASIL_RAW/acidentes', f'acidentes{i}.csv')\n",
    "        dst = os.path.join('editedDataset/UTF_acidentes', f'utf_acidentes{i}.csv')\n",
    "        # The \\ operator is useful to truncate the writing of the code in multiple line for\n",
    "        # improving the readability of the code\n",
    "        with open(src, \"r\", encoding=\"iso-8859-1\", errors=\"strict\") as fin, \\\n",
    "            open(dst, \"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "            for line in fin:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30342a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_df_full = []\n",
    "files_to_inspect = ['2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
    "\n",
    "for y in files_to_inspect:\n",
    "    # Step 1: Importing the dataset of year y\n",
    "    df_full = pd.read_csv(os.path.join('editedDataset/UTF_acidentes', f'utf_acidentes{y}.csv'), sep=\";\", dtype={22: \"string\", 23:\"string\", 25:\"string\"})\n",
    "\n",
    "    # Step 2: Translation of the category names in English\n",
    "    # Taking the first attributes until road_delineation\n",
    "\n",
    "    # Translation of the category names in English\n",
    "    # Taking the first attributes until road_delineation\n",
    "\n",
    "    en_attrNames_head = (df.loc[:,:'road_delineation'].columns).tolist()\n",
    "    # We need the previous labels\n",
    "    df_full_columns = df_full.columns.tolist()\n",
    "\n",
    "    # With this selection we're sure to substitute only the names in the first part\n",
    "    df_full_columns[2:17] = en_attrNames_head\n",
    "    # Writing the translation for the central part of the columns\n",
    "    df_full_columns[17:26] = ['without_passengers', 'veichle_id', 'veichle_type', 'veichle_brand', 'veichle_manufacturing_year', 'person_kind', 'person_condition', 'person_age', 'person_sex']\n",
    "    df_full_columns[26:30] = ['person_is_unharmed', 'person_is_slightly_injured', 'person_is_severely_injured', 'person_is_dead']\n",
    "    df_full_columns[33] = 'police_station'\n",
    "    # Applying the translation to the DataFrame\n",
    "    df_full.columns = df_full_columns\n",
    "\n",
    "    # Step 3: Translating the instances values in English (attribute per attribute)\n",
    "\n",
    "    without_passengers_map = {\n",
    "        'Não': 'No',\n",
    "        'Sim': 'Yes'\n",
    "    }\n",
    "    df_full[\"without_passengers\"] = df_full[\"without_passengers\"].replace(without_passengers_map)\n",
    "\n",
    "    vehicle_type_map = {\n",
    "        \"Automóvel\": \"Car\",\n",
    "        \"Motocicleta\": \"Motorcycle\",\n",
    "        \"Semireboque\": \"Semi-trailer\",\n",
    "        \"Caminhonete\": \"Pickup truck\",\n",
    "        \"Caminhão-trator\": \"Tractor-trailer truck\",\n",
    "        \"Caminhão\": \"Truck\",\n",
    "        \"Ônibus\": \"Bus\",\n",
    "        \"Camioneta\": \"Van\",\n",
    "        \"Motoneta\": \"Scooter\",\n",
    "        \"Utilitário\": \"Utility vehicle\",\n",
    "        \"Bicicleta\": \"Bicycle\",\n",
    "        \"Micro-ônibus\": \"Minibus\",\n",
    "        \"Reboque\": \"Trailer\",\n",
    "        \"Outros\": \"Others\",\n",
    "        \"Ciclomotor\": \"Moped\",\n",
    "        \"Carroça-charrete\": \"Cart-wagon\",\n",
    "        \"Trator de rodas\": \"Wheeled tractor\",\n",
    "        \"Motor-casa\": \"Motorhome\",\n",
    "        \"Triciclo\": \"Tricycle\",\n",
    "        \"Trem-bonde\": \"Tram\",\n",
    "        \"Trator de esteira\": \"Crawler tractor\",\n",
    "        \"Trator misto\": \"Backhoe loader\",\n",
    "        \"Carro de mão\": \"Wheelbarrow\",\n",
    "        \"Chassi-plataforma\": \"Chassis platform\",\n",
    "        \"Quadriciclo\": \"Quadricycle\"\n",
    "    }\n",
    "    df_full[\"veichle_type\"] = df_full[\"veichle_type\"].replace(vehicle_type_map)\n",
    "\n",
    "    df_full['veichle_brand'] = df_full[\"veichle_brand\"].replace({\n",
    "        \"Não Informado/Não Informado\": pd.NA,\n",
    "        \"NA/NA\": pd.NA\n",
    "    })\n",
    "\n",
    "    df_full['veichle_manufacturing_year'] = df_full[\"veichle_manufacturing_year\"].replace(0,pd.NA)\n",
    "\n",
    "    person_kind_map = {\n",
    "        'Condutor': 'Driver',\n",
    "        'Passageiro': 'Passenger',\n",
    "        'Pedestre': 'Pedestrian',\n",
    "        'Testemunha': 'Withness',\n",
    "        'Cavaleiro': 'Knight'\n",
    "    }\n",
    "    df_full[\"person_kind\"] = df_full[\"person_kind\"].replace(person_kind_map)\n",
    "\n",
    "    person_sex_map = {\n",
    "        'Masculino': 'M',\n",
    "        'Feminino': 'F',\n",
    "        'Não Informado': pd.NA,\n",
    "        'Ignorado': pd.NA\n",
    "    }\n",
    "    df_full[\"person_sex\"] = df_full[\"person_sex\"].replace(person_sex_map)\n",
    "\n",
    "    person_condition_map = {\n",
    "        'Ileso': 'Unharmed',\n",
    "        'Lesões Leves': 'Slightly Injured',\n",
    "        'Lesões Graves': 'Severely Injured',\n",
    "        'Não Informado': pd.NA,\n",
    "        'Óbito': 'Dead'\n",
    "    }\n",
    "    df_full[\"person_condition\"] = df_full[\"person_condition\"].replace(person_condition_map)\n",
    "\n",
    "    # Step 4: Converting latitude, longitude from object to float64\n",
    "    # We need also to round to 5 digits in order to make the join operation successful\n",
    "    attr_to_conv = [\"latitude\", \"longitude\"]\n",
    "    for attr in attr_to_conv:\n",
    "        df_full[attr] = df_full[attr].astype(str).str.replace(\",\", \".\").astype(float).round(5)\n",
    "        df[attr] = df[attr].round(5)\n",
    "\n",
    "    # Step 5: We sort by inverse_data, hour, city (in place)\n",
    "    df.sort_values(by=['inverse_data', 'hour', 'city'], inplace=True)\n",
    "    df_full.sort_values(by=['inverse_data', 'hour', 'city'], inplace=True)\n",
    "\n",
    "    # Step 6: From df_full we remove all the duplicate attributes that coincides in both the level of granularity\n",
    "    indexes = {0,2} # we drop all the head attributes exception made the ones useful for the join\n",
    "    en_attrNames_head = [x for i,x in enumerate(en_attrNames_head) if i not in indexes]\n",
    "    df_full = df_full.drop(columns=en_attrNames_head)\n",
    "\n",
    "    # Step 7: We join the two tables with different granularity\n",
    "    df_joined = pd.merge(df, df_full, on=['latitude', 'longitude', 'inverse_data', 'hour'])\n",
    "    arr_df_full.append(df_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e2e1fb",
   "metadata": {},
   "source": [
    "`arr_df_full` is an array of DataFrames. In each DataFrame one instance is a person or a withness involved in an accidents, but also has the details about the accident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3df734",
   "metadata": {},
   "source": [
    "Now, let's produce the file `INTEGRATION_brasilEnglishFull.csv`, which is the result of the join operation we've performed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d435976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "out_dir = 'editedDataset'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "file_path = os.path.join(out_dir, 'INTEGRATION_brasilEnglishFull.csv')\n",
    "if not os.path.exists(file_path):\n",
    "    df_all = pd.concat(arr_df_full, ignore_index=True)\n",
    "    df_all.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a9d85",
   "metadata": {},
   "source": [
    "### Road feature integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd0fbf",
   "metadata": {},
   "source": [
    "Now, we'll exploit Overpass API to gather some additional details about the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1aa491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_indexes = [0,100000]\n",
    "end_indexes = [100,100100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e27fd232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 24622\n",
      "Processing index: 24623\n",
      "Processing index: 24629\n",
      "Processing index: 24620\n",
      "Processing index: 89098\n",
      "Processing index: 25311\n",
      "Processing index: 24674\n",
      "Processing index: 24625\n",
      "Processing index: 24621\n",
      "Processing index: 24671\n",
      "Processing index: 24626\n",
      "Processing index: 24634\n",
      "Processing index: 24643\n",
      "Processing index: 24628\n",
      "Processing index: 26949\n",
      "Processing index: 24633\n",
      "Processing index: 24624\n",
      "Processing index: 25213\n",
      "Processing index: 24664\n",
      "Processing index: 31823\n",
      "Processing index: 1\n",
      "Processing index: 24630\n",
      "Processing index: 24635\n",
      "Processing index: 24637\n",
      "Processing index: 24642\n",
      "Processing index: 24638\n",
      "Processing index: 24631\n",
      "Processing index: 24627\n",
      "Processing index: 24679\n",
      "Processing index: 0\n",
      "Processing index: 24669\n",
      "Processing index: 24632\n",
      "Processing index: 24636\n",
      "Processing index: 1262\n",
      "Processing index: 24644\n",
      "Processing index: 24639\n",
      "Processing index: 24641\n",
      "Processing index: 48780\n",
      "Processing index: 24640\n",
      "Processing index: 24646\n",
      "Processing index: 24656\n",
      "Processing index: 24657\n",
      "Processing index: 24645\n",
      "Processing index: 24667\n",
      "Processing index: 24698\n",
      "Processing index: 24699\n",
      "Processing index: 24655\n",
      "Processing index: 2\n",
      "Processing index: 24647\n",
      "Processing index: 24650\n",
      "Processing index: 24649\n",
      "Processing index: 24654\n",
      "Processing index: 24648\n",
      "Processing index: 24653\n",
      "Processing index: 24660\n",
      "Processing index: 24652\n",
      "Processing index: 72243\n",
      "Processing index: 29277\n",
      "Processing index: 25897\n",
      "Processing index: 24651\n",
      "Processing index: 24662\n",
      "Processing index: 24663\n",
      "Processing index: 24666\n",
      "Processing index: 24680\n",
      "Processing index: 25165\n",
      "Processing index: 24658\n",
      "Processing index: 24670\n",
      "Processing index: 24673\n",
      "Processing index: 24661\n",
      "Processing index: 24659\n",
      "Processing index: 24697\n",
      "Processing index: 3\n",
      "Processing index: 25205\n",
      "Processing index: 24672\n",
      "Processing index: 24690\n",
      "Processing index: 24683\n",
      "Processing index: 24677\n",
      "Processing index: 24668\n",
      "Processing index: 26213\n",
      "Processing index: 5\n",
      "Processing index: 24511\n",
      "Processing index: 25488\n",
      "Processing index: 24675\n",
      "Processing index: 24713\n",
      "Processing index: 24971\n",
      "Processing index: 24678\n",
      "Processing index: 24839\n",
      "Processing index: 27609\n",
      "Processing index: 24676\n",
      "Processing index: 26566\n",
      "Processing index: 24681\n",
      "Processing index: 24735\n",
      "Processing index: 24689\n",
      "Processing index: 9\n",
      "Processing index: 24684\n",
      "Processing index: 24701\n",
      "Processing index: 24687\n",
      "Processing index: 24714\n",
      "Processing index: 24708\n",
      "Processing index: 24691\n",
      "Processing index: 92195\n",
      "Processing index: 92196\n",
      "Processing index: 93668\n",
      "Processing index: 92197\n",
      "Processing index: 115032\n",
      "Processing index: 115038\n",
      "Processing index: 115034\n",
      "Processing index: 92198\n",
      "Processing index: 115033\n",
      "Processing index: 115051\n",
      "Processing index: 115041\n",
      "Processing index: 115060\n",
      "Processing index: 115043\n",
      "Processing index: 92200\n",
      "Processing index: 92199\n",
      "Processing index: 115047\n",
      "Processing index: 92201\n",
      "Processing index: 92224\n",
      "Processing index: 115039\n",
      "Processing index: 115045\n",
      "Processing index: 115066\n",
      "Processing index: 115063\n",
      "Processing index: 115046\n",
      "Processing index: 115056\n",
      "Processing index: 115049\n",
      "Processing index: 115042\n",
      "Processing index: 115040\n",
      "Processing index: 115044\n",
      "Processing index: 126237\n",
      "Processing index: 115048\n",
      "Processing index: 115092\n",
      "Processing index: 92205\n",
      "Processing index: 115062\n",
      "Processing index: 115050\n",
      "Processing index: 115052\n",
      "Processing index: 115053\n",
      "Processing index: 115054\n",
      "Processing index: 115057\n",
      "Processing index: 92219\n",
      "Processing index: 115055\n",
      "Processing index: 92202\n",
      "Processing index: 115058\n",
      "Processing index: 115073\n",
      "Processing index: 115059\n",
      "Processing index: 115061\n",
      "Processing index: 115067\n",
      "Processing index: 115064\n",
      "Processing index: 115065\n",
      "Processing index: 115104\n",
      "Processing index: 92206\n",
      "Processing index: 92208\n",
      "Processing index: 92228\n",
      "Processing index: 92225\n",
      "Processing index: 115070\n",
      "Processing index: 115076\n",
      "Processing index: 115068\n",
      "Processing index: 92209\n",
      "Processing index: 115083\n",
      "Processing index: 115077\n",
      "Processing index: 115069\n",
      "Processing index: 115071\n",
      "Processing index: 92212\n",
      "Processing index: 115120\n",
      "Processing index: 115072\n",
      "Processing index: 115075\n",
      "Processing index: 115080\n",
      "Processing index: 115096\n",
      "Processing index: 92213\n",
      "Processing index: 92223\n",
      "Processing index: 115110\n",
      "Processing index: 115078\n",
      "Processing index: 136457\n",
      "Processing index: 115106\n",
      "Processing index: 115082\n",
      "Processing index: 115079\n",
      "Processing index: 118541\n",
      "Processing index: 115101\n",
      "Processing index: 115081\n",
      "Processing index: 115087\n",
      "Processing index: 115090\n",
      "Processing index: 115102\n",
      "Processing index: 115084\n",
      "Processing index: 115085\n",
      "Processing index: 115091\n",
      "Processing index: 115094\n",
      "Processing index: 115107\n",
      "Processing index: 115089\n",
      "Processing index: 115086\n",
      "Processing index: 107543\n",
      "Processing index: 115088\n",
      "Processing index: 115095\n",
      "Processing index: 107547\n",
      "Processing index: 115093\n",
      "Processing index: 115100\n",
      "Processing index: 92220\n",
      "Processing index: 92217\n",
      "Processing index: 92214\n",
      "Processing index: 119972\n",
      "Processing index: 115112\n",
      "Processing index: 115108\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "url = \"http://overpass-api.de/api/interpreter\"\n",
    "overpass_dfs = []\n",
    "\n",
    "for start_index, end_index in zip(start_indexes, end_indexes):\n",
    "    df_coords_unique = df.iloc[start_index:end_index+1].loc[:,'latitude':'longitude'].drop_duplicates()\n",
    "    df_coords_unique\n",
    "\n",
    "    tags = []\n",
    "    for index, row in df_coords_unique.iterrows():\n",
    "        latitude = row['latitude']\n",
    "        longitude = row['longitude']\n",
    "\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:10];\n",
    "        way(around:5,{latitude},{longitude})[highway];\n",
    "        out tags;\n",
    "        \"\"\"\n",
    "\n",
    "        # retry loop\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, params={'data': query}, timeout=30)\n",
    "                if response.status_code == 200:\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Attempt {attempt+1}/{max_retries} failed with status {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Tentativo {attempt+1}/{max_retries} fallito con errore: {e}\")\n",
    "            sleep(2)\n",
    "        else:\n",
    "            # se esco dal ciclo senza break, vuol dire che tutti i tentativi sono falliti\n",
    "            print(f\"All attempts failed at {index}, skipping...\")\n",
    "            continue\n",
    "        response = requests.get(url, params={'data': query})\n",
    "        \n",
    "        print(f\"Processing index: {index}\")\n",
    "        data = response.json()\n",
    "        if data[\"elements\"]:\n",
    "            elements_arr = data[\"elements\"]\n",
    "            for elem in elements_arr:\n",
    "                if elem[\"tags\"]:\n",
    "                    dict = elem[\"tags\"]\n",
    "                    # I want to keep only: maxspeed, lanes, name, operator, toll, surface, oneway\n",
    "                    # I use a list comprehesion\n",
    "                    keep = {\"maxspeed\", \"lanes\", \"name\", \"operator\", \"toll\", \"surface\", \"oneway\"}\n",
    "                    dict = {k: v for k, v in elem[\"tags\"].items() if k in keep}\n",
    "                    dict['latitude'] = latitude\n",
    "                    dict['longitude'] = longitude\n",
    "                    tags.append(dict)\n",
    "\n",
    "    overpass_df = pd.DataFrame(tags)\n",
    "    overpass_dfs.append(overpass_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "192b10ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overpass_dfs[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "223b75a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overpass_dfs[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4991b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overpass_dfs_clean = []\n",
    "for overpass_df in overpass_dfs:\n",
    "    overpass_df_clean = overpass_df.dropna(subset=['lanes', 'oneway', ])\n",
    "    overpass_dfs_clean.append(overpass_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0beb5878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overpass_dfs_clean[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2968ac63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overpass_dfs_clean[1].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a9be8",
   "metadata": {},
   "source": [
    "Since in both cases we obtained that from 100 instances we begin from, only 25-30 of them have associated information concerning the speedlimit and other road features I decide to not consider this aspect in the project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
