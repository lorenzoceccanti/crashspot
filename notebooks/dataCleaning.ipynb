{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25afb80",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Data Mining and Machine Learning </h2>\n",
    "<h3 align=\"center\"> Final Project </h3>\n",
    "<h2 align=\"center\"> <b> <i> CrashSpot </i> </b> </h2>\n",
    "<h4 align=\"center\"> Lorenzo Ceccanti matr. 564490 </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e183cf7",
   "metadata": {},
   "source": [
    "### <b> Data Cleaning </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14486a2",
   "metadata": {},
   "source": [
    "In the _dataExploration_ notebook, we saw that on the 463152 entries only `police_station` attribute has a large quantity of null variables.\n",
    "\n",
    "However, let's make a check for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f93632",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"../dataset/BRASIL_AGGR\"\n",
    "editedDataset_folder = \"../editedDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76da9798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "df_clean = pd.read_csv(os.path.join(dataset_folder, 'accidents_2017_to_2023_english_BRASIL.csv'))\n",
    "df_clean.rename(columns={\"ignored\": \"unharmed\"}, inplace=True)\n",
    "\n",
    "total_tuples = df_clean.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1929fd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "314959fe-d6e9-4d35-947e-1e4a875583ef",
       "rows": [
        [
         "inverse_data",
         "0"
        ],
        [
         "week_day",
         "0"
        ],
        [
         "hour",
         "0"
        ],
        [
         "state",
         "0"
        ],
        [
         "road_id",
         "990"
        ],
        [
         "km",
         "990"
        ],
        [
         "city",
         "0"
        ],
        [
         "cause_of_accident",
         "0"
        ],
        [
         "type_of_accident",
         "0"
        ],
        [
         "victims_condition",
         "0"
        ],
        [
         "weather_timestamp",
         "0"
        ],
        [
         "road_direction",
         "0"
        ],
        [
         "wheather_condition",
         "0"
        ],
        [
         "road_type",
         "0"
        ],
        [
         "road_delineation",
         "0"
        ],
        [
         "people",
         "0"
        ],
        [
         "deaths",
         "0"
        ],
        [
         "slightly_injured",
         "0"
        ],
        [
         "severely_injured",
         "0"
        ],
        [
         "uninjured",
         "0"
        ],
        [
         "unharmed",
         "0"
        ],
        [
         "total_injured",
         "0"
        ],
        [
         "vehicles_involved",
         "0"
        ],
        [
         "latitude",
         "0"
        ],
        [
         "longitude",
         "0"
        ],
        [
         "regional",
         "10"
        ],
        [
         "police_station",
         "1310"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 27
       }
      },
      "text/plain": [
       "inverse_data             0\n",
       "week_day                 0\n",
       "hour                     0\n",
       "state                    0\n",
       "road_id                990\n",
       "km                     990\n",
       "city                     0\n",
       "cause_of_accident        0\n",
       "type_of_accident         0\n",
       "victims_condition        0\n",
       "weather_timestamp        0\n",
       "road_direction           0\n",
       "wheather_condition       0\n",
       "road_type                0\n",
       "road_delineation         0\n",
       "people                   0\n",
       "deaths                   0\n",
       "slightly_injured         0\n",
       "severely_injured         0\n",
       "uninjured                0\n",
       "unharmed                 0\n",
       "total_injured            0\n",
       "vehicles_involved        0\n",
       "latitude                 0\n",
       "longitude                0\n",
       "regional                10\n",
       "police_station        1310\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff39f31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of instances: 463152\n",
      "Number of instances lost: 1310\n",
      "Estimation of the percentage of instances lost on the entire dataset: 0.28%\n"
     ]
    }
   ],
   "source": [
    "tuple_to_drop = df_clean['police_station'].isna().sum()\n",
    "percentage = (tuple_to_drop / total_tuples) * 100\n",
    "print(f'Total number of instances: {total_tuples}')\n",
    "print(f'Number of instances lost: {tuple_to_drop}')\n",
    "print(f'Estimation of the percentage of instances lost on the entire dataset: {percentage:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903f791",
   "metadata": {},
   "source": [
    "Because the percentage of instances lost on the entire dataset is 0.28%, I decide to remove the instances in which `police_station` has null value.\n",
    "\n",
    "For the same reasons, we remove also the instances in which `regional`, `road_id` and `km` have null values in the respective attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d500769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances dropped: 2223\n",
      "Real percentage of instances lost on the total number of instances available: 0.48%\n"
     ]
    }
   ],
   "source": [
    "# The subset parameter allows to specify in which columns we consider the presence of null instances. \n",
    "df_clean = df_clean.dropna(subset=['police_station', 'regional', 'road_id', 'km'])\n",
    "print(f'Number of instances dropped: {total_tuples - df_clean.shape[0]}')\n",
    "eff_percentage = ((total_tuples - df_clean.shape[0])/total_tuples)*100\n",
    "print(f'Real percentage of instances lost on the total number of instances available: {eff_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46672752",
   "metadata": {},
   "source": [
    "Previously, during data exploration, we saw that 40 instances had _Not informed_ as value for `type_of_accident`.\n",
    "\n",
    "Let's check if there still exists in the cleaned version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e315660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_tmp = df_clean.replace('Not informed', np.nan)\n",
    "df_tmp.loc[:,'type_of_accident'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ecebb",
   "metadata": {},
   "source": [
    "Yes, there are still there. We proceed to remove such instances from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27750723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.replace('Not informed', np.nan).dropna(subset=['type_of_accident'])\n",
    "df_clean.loc[:,'type_of_accident'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52393c93",
   "metadata": {},
   "source": [
    "### Counting the records having a no-sense value for latitude and longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f3b61",
   "metadata": {},
   "source": [
    "A latitude value has sense if it's between -90 and +90, while a longitude value has sense if it's between -180 and +180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4238c139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_invalid = df_clean[\n",
    "    (df_clean['latitude'] > 90) | (df_clean['latitude'] < -90) |\n",
    "    (df_clean['longitude'] > 180) | (df_clean['longitude'] < -180)\n",
    "]\n",
    "tuple_to_drop = df_invalid.shape[0]\n",
    "tuple_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f156b",
   "metadata": {},
   "source": [
    "Since there are only 32 instances affected by this problem, we decide to simply remove those instances from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37767b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 32 instances\n"
     ]
    }
   ],
   "source": [
    "old_instances_count = df_clean.shape[0]\n",
    "# We have available the original indexes used for df_clean in df_invalid, so we can use the drop method\n",
    "df_clean = df_clean.drop(df_invalid.index)\n",
    "new_instances_count = df_clean.shape[0]\n",
    "print(f'Removed {old_instances_count - new_instances_count} instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f895cc",
   "metadata": {},
   "source": [
    "Let's write on file the cleaned version of BRASIL_AGGR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc833f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "out_dir = editedDataset_folder\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "file_path = os.path.join(out_dir, 'CLEANED_brasilEnglishAggr.csv')\n",
    "df_clean.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
