{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6841f938",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Data Mining and Machine Learning </h2>\n",
    "<h3 align=\"center\"> Final Project </h3>\n",
    "<h2 align=\"center\"> <b> <i> CrashSpot </i> </b> </h2>\n",
    "<h4 align=\"center\"> Lorenzo Ceccanti matr. 564490 </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9e5c5",
   "metadata": {},
   "source": [
    "### <b> Data Integration </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e5569f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "editedDataset_folder = \"../editedDataset\"\n",
    "dataset_folder = \"../dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d891239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "week_day",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hour",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "state",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "road_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "km",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "city",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cause_of_accident",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type_of_accident",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "victims_condition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "weather_timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "road_direction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "weather_condition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "road_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "people",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "deaths",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "slightly_injured",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "severely_injured",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "uninjured",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "unharmed",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_injured",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "vehicles_involved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "regional",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "police_station",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "c6389bee-5591-49b3-8f5c-77f880d158d9",
       "rows": [
        [
         "0",
         "2017-01-01",
         "sunday",
         "01:45:00",
         "RS",
         "116",
         "34,9",
         "VACARIA",
         "Mechanical loss/defect of vehicle",
         "Rear-end collision",
         "With injured victims",
         "Night",
         "Decreasing",
         "Clear sky",
         "Simple",
         "6",
         "0",
         "4",
         "0",
         "2",
         "0",
         "4",
         "2",
         "-28.50712",
         "-50.94118",
         "SPRF-RS",
         "DEL05-RS"
        ],
        [
         "1",
         "2017-01-01",
         "sunday",
         "01:00:00",
         "PR",
         "376",
         "636",
         "TIJUCAS DO SUL",
         "Incompatible velocity",
         "Run-off-road",
         "With dead victims",
         "Night",
         "Increasing",
         "Drizzle",
         "Double",
         "2",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "2",
         "-25.754",
         "-49.1266",
         "SPRF-PR",
         "DEL01-PR"
        ],
        [
         "2",
         "2017-01-01",
         "sunday",
         "04:40:00",
         "BA",
         "101",
         "65",
         "ENTRE RIOS",
         "Driver was sleeping",
         "Head-on collision",
         "With dead victims",
         "Sunrise",
         "Decreasing",
         "Cloudy",
         "Simple",
         "5",
         "1",
         "1",
         "1",
         "2",
         "0",
         "2",
         "2",
         "-11.9618",
         "-38.0953",
         "SPRF-BA",
         "DEL01-BA"
        ]
       ],
       "shape": {
        "columns": 26,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>week_day</th>\n",
       "      <th>hour</th>\n",
       "      <th>state</th>\n",
       "      <th>road_id</th>\n",
       "      <th>km</th>\n",
       "      <th>city</th>\n",
       "      <th>cause_of_accident</th>\n",
       "      <th>type_of_accident</th>\n",
       "      <th>victims_condition</th>\n",
       "      <th>weather_timestamp</th>\n",
       "      <th>road_direction</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>road_type</th>\n",
       "      <th>people</th>\n",
       "      <th>deaths</th>\n",
       "      <th>slightly_injured</th>\n",
       "      <th>severely_injured</th>\n",
       "      <th>uninjured</th>\n",
       "      <th>unharmed</th>\n",
       "      <th>total_injured</th>\n",
       "      <th>vehicles_involved</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>regional</th>\n",
       "      <th>police_station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>sunday</td>\n",
       "      <td>01:45:00</td>\n",
       "      <td>RS</td>\n",
       "      <td>116</td>\n",
       "      <td>34,9</td>\n",
       "      <td>VACARIA</td>\n",
       "      <td>Mechanical loss/defect of vehicle</td>\n",
       "      <td>Rear-end collision</td>\n",
       "      <td>With injured victims</td>\n",
       "      <td>Night</td>\n",
       "      <td>Decreasing</td>\n",
       "      <td>Clear sky</td>\n",
       "      <td>Simple</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.50712</td>\n",
       "      <td>-50.94118</td>\n",
       "      <td>SPRF-RS</td>\n",
       "      <td>DEL05-RS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>sunday</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>PR</td>\n",
       "      <td>376</td>\n",
       "      <td>636</td>\n",
       "      <td>TIJUCAS DO SUL</td>\n",
       "      <td>Incompatible velocity</td>\n",
       "      <td>Run-off-road</td>\n",
       "      <td>With dead victims</td>\n",
       "      <td>Night</td>\n",
       "      <td>Increasing</td>\n",
       "      <td>Drizzle</td>\n",
       "      <td>Double</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-25.75400</td>\n",
       "      <td>-49.12660</td>\n",
       "      <td>SPRF-PR</td>\n",
       "      <td>DEL01-PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>sunday</td>\n",
       "      <td>04:40:00</td>\n",
       "      <td>BA</td>\n",
       "      <td>101</td>\n",
       "      <td>65</td>\n",
       "      <td>ENTRE RIOS</td>\n",
       "      <td>Driver was sleeping</td>\n",
       "      <td>Head-on collision</td>\n",
       "      <td>With dead victims</td>\n",
       "      <td>Sunrise</td>\n",
       "      <td>Decreasing</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-11.96180</td>\n",
       "      <td>-38.09530</td>\n",
       "      <td>SPRF-BA</td>\n",
       "      <td>DEL01-BA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date week_day      hour state  road_id    km            city  \\\n",
       "0  2017-01-01   sunday  01:45:00    RS      116  34,9         VACARIA   \n",
       "1  2017-01-01   sunday  01:00:00    PR      376   636  TIJUCAS DO SUL   \n",
       "2  2017-01-01   sunday  04:40:00    BA      101    65      ENTRE RIOS   \n",
       "\n",
       "                   cause_of_accident    type_of_accident  \\\n",
       "0  Mechanical loss/defect of vehicle  Rear-end collision   \n",
       "1              Incompatible velocity        Run-off-road   \n",
       "2                Driver was sleeping   Head-on collision   \n",
       "\n",
       "      victims_condition weather_timestamp road_direction weather_condition  \\\n",
       "0  With injured victims             Night     Decreasing         Clear sky   \n",
       "1     With dead victims             Night     Increasing           Drizzle   \n",
       "2     With dead victims           Sunrise     Decreasing            Cloudy   \n",
       "\n",
       "  road_type  people  deaths  slightly_injured  severely_injured  uninjured  \\\n",
       "0    Simple       6       0                 4                 0          2   \n",
       "1    Double       2       1                 0                 0          1   \n",
       "2    Simple       5       1                 1                 1          2   \n",
       "\n",
       "   unharmed  total_injured  vehicles_involved  latitude  longitude regional  \\\n",
       "0         0              4                  2 -28.50712  -50.94118  SPRF-RS   \n",
       "1         0              0                  2 -25.75400  -49.12660  SPRF-PR   \n",
       "2         0              2                  2 -11.96180  -38.09530  SPRF-BA   \n",
       "\n",
       "  police_station  \n",
       "0       DEL05-RS  \n",
       "1       DEL01-PR  \n",
       "2       DEL01-BA  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(os.path.join(editedDataset_folder, 'CLEANED_brasilEnglishAggr.csv'))\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88175036",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00ed6e",
   "metadata": {},
   "source": [
    "Let's inspect the dataset in which the granularity is per occupant, which will be useful for the second part of my project.\n",
    "\n",
    "This dataset is not in English, so first it requires a little bit of translation\n",
    "\n",
    "The source of data however it's the same. Each file contains different years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857f3db",
   "metadata": {},
   "source": [
    "<b> Problem </b>: If I try to import directly the dataset `BRASIL_RAW` we obtain an UnicodeDecodeError. We discover that the encoding for the dataset is not UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68ddf05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding for acidentes2017: ISO-8859-1\n",
      "Encoding for acidentes2018: ISO-8859-1\n",
      "Encoding for acidentes2019: ISO-8859-1\n",
      "Encoding for acidentes2020: ISO-8859-1\n",
      "Encoding for acidentes2021: ISO-8859-1\n",
      "Encoding for acidentes2022: ISO-8859-1\n",
      "Encoding for acidentes2023: ISO-8859-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chardet\n",
    "\n",
    "for i in range(2017,2024):\n",
    "    with open(os.path.join(f'{dataset_folder}/BRASIL_RAW/acidentes', f'acidentes{i}.csv'), 'rb') as f:\n",
    "        result = chardet.detect(f.read(10000))  # leggi i primi 10k byte\n",
    "        print(f'Encoding for acidentes{i}: ' + result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fb55cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This little script converts the original brasil_raw into UTF-8 encoding\n",
    "\n",
    "# Checking if the new directory we want to create already exists\n",
    "out_dir = f\"{editedDataset_folder}/UTF_acidentes\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "    for i in range(2017,2024):\n",
    "        src = os.path.join(f'{dataset_folder}/BRASIL_RAW/acidentes', f'acidentes{i}.csv')\n",
    "        dst = os.path.join(f'{editedDataset_folder}/UTF_acidentes', f'utf_acidentes{i}.csv')\n",
    "        # The \\ operator is useful to truncate the writing of the code in multiple line for\n",
    "        # improving the readability of the code\n",
    "        with open(src, \"r\", encoding=\"iso-8859-1\", errors=\"strict\") as fin, \\\n",
    "            open(dst, \"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "            for line in fin:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30342a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_df_full = []\n",
    "files_to_inspect = ['2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
    "\n",
    "for y in files_to_inspect:\n",
    "    # Step 1: Importing the dataset of year y\n",
    "    df_full = pd.read_csv(os.path.join(f'{editedDataset_folder}/UTF_acidentes', f'utf_acidentes{y}.csv'), sep=\";\", dtype={22: \"string\", 23:\"string\", 25:\"string\"})\n",
    "\n",
    "    # Step 2: Translation of the category names in English\n",
    "    # Taking the first attributes until road_delineation\n",
    "\n",
    "    # Translation of the category names in English\n",
    "    # Taking the first attributes until road_delineation\n",
    "\n",
    "    en_attrNames_head = (df.loc[:,:'people'].columns).tolist()\n",
    "    # We need the previous labels\n",
    "    df_full_columns = df_full.columns.tolist()\n",
    "\n",
    "    # With this selection we're sure to substitute only the names in the first part\n",
    "    df_full_columns[2:17] = en_attrNames_head\n",
    "    # Writing the translation for the central part of the columns\n",
    "    df_full_columns[17:26] = ['without_passengers', 'veichle_id', 'veichle_type', 'veichle_brand', 'veichle_manufacturing_year', 'person_kind', 'person_condition', 'person_age', 'person_sex']\n",
    "    df_full_columns[26:30] = ['person_is_unharmed', 'person_is_slightly_injured', 'person_is_severely_injured', 'person_is_dead']\n",
    "    df_full_columns[32] = 'police_station'\n",
    "    # Applying the translation to the DataFrame\n",
    "    df_full.columns = df_full_columns\n",
    "\n",
    "    # Step 3: Translating the instances values in English (attribute per attribute)\n",
    "\n",
    "    without_passengers_map = {\n",
    "        'Não': 'No',\n",
    "        'Sim': 'Yes'\n",
    "    }\n",
    "    df_full[\"without_passengers\"] = df_full[\"without_passengers\"].replace(without_passengers_map)\n",
    "\n",
    "    vehicle_type_map = {\n",
    "        \"Automóvel\": \"Car\",\n",
    "        \"Motocicleta\": \"Motorcycle\",\n",
    "        \"Semireboque\": \"Semi-trailer\",\n",
    "        \"Caminhonete\": \"Pickup truck\",\n",
    "        \"Caminhão-trator\": \"Tractor-trailer truck\",\n",
    "        \"Caminhão\": \"Truck\",\n",
    "        \"Ônibus\": \"Bus\",\n",
    "        \"Camioneta\": \"Van\",\n",
    "        \"Motoneta\": \"Scooter\",\n",
    "        \"Utilitário\": \"Utility vehicle\",\n",
    "        \"Bicicleta\": \"Bicycle\",\n",
    "        \"Micro-ônibus\": \"Minibus\",\n",
    "        \"Reboque\": \"Trailer\",\n",
    "        \"Outros\": \"Others\",\n",
    "        \"Ciclomotor\": \"Moped\",\n",
    "        \"Carroça-charrete\": \"Cart-wagon\",\n",
    "        \"Trator de rodas\": \"Wheeled tractor\",\n",
    "        \"Motor-casa\": \"Motorhome\",\n",
    "        \"Triciclo\": \"Tricycle\",\n",
    "        \"Trem-bonde\": \"Tram\",\n",
    "        \"Trator de esteira\": \"Crawler tractor\",\n",
    "        \"Trator misto\": \"Backhoe loader\",\n",
    "        \"Carro de mão\": \"Wheelbarrow\",\n",
    "        \"Chassi-plataforma\": \"Chassis platform\",\n",
    "        \"Quadriciclo\": \"Quadricycle\"\n",
    "    }\n",
    "    df_full[\"veichle_type\"] = df_full[\"veichle_type\"].replace(vehicle_type_map)\n",
    "\n",
    "    df_full['veichle_brand'] = df_full[\"veichle_brand\"].replace({\n",
    "        \"Não Informado/Não Informado\": pd.NA,\n",
    "        \"NA/NA\": pd.NA\n",
    "    })\n",
    "\n",
    "    df_full['veichle_manufacturing_year'] = df_full[\"veichle_manufacturing_year\"].replace(0,pd.NA)\n",
    "\n",
    "    person_kind_map = {\n",
    "        'Condutor': 'Driver',\n",
    "        'Passageiro': 'Passenger',\n",
    "        'Pedestre': 'Pedestrian',\n",
    "        'Testemunha': 'Withness',\n",
    "        'Cavaleiro': 'Knight'\n",
    "    }\n",
    "    df_full[\"person_kind\"] = df_full[\"person_kind\"].replace(person_kind_map)\n",
    "\n",
    "    person_sex_map = {\n",
    "        'Masculino': 'M',\n",
    "        'Feminino': 'F',\n",
    "        'Não Informado': pd.NA,\n",
    "        'Ignorado': pd.NA\n",
    "    }\n",
    "    df_full[\"person_sex\"] = df_full[\"person_sex\"].replace(person_sex_map)\n",
    "\n",
    "    person_condition_map = {\n",
    "        'Ileso': 'Unharmed',\n",
    "        'Lesões Leves': 'Slightly Injured',\n",
    "        'Lesões Graves': 'Severely Injured',\n",
    "        'Não Informado': pd.NA,\n",
    "        'Óbito': 'Dead'\n",
    "    }\n",
    "    df_full[\"person_condition\"] = df_full[\"person_condition\"].replace(person_condition_map)\n",
    "\n",
    "    # Step 4: Converting latitude, longitude from object to float64\n",
    "    # We need also to round to 5 digits in order to make the join operation successful\n",
    "    attr_to_conv = [\"latitude\", \"longitude\"]\n",
    "    for attr in attr_to_conv:\n",
    "        df_full[attr] = df_full[attr].astype(str).str.replace(\",\", \".\").astype(float).round(5)\n",
    "        df[attr] = df[attr].round(5)\n",
    "\n",
    "    # Step 5: We sort by inverse_data, hour, city (in place)\n",
    "    df.sort_values(by=['date', 'hour', 'city'], inplace=True)\n",
    "    df_full.sort_values(by=['date', 'hour', 'city'], inplace=True)\n",
    "\n",
    "    # Step 6: From df_full we remove all the duplicate attributes that coincides in both the level of granularity\n",
    "    indexes = {0,2} # we drop all the head attributes exception made the ones useful for the join\n",
    "    en_attrNames_head = [x for i,x in enumerate(en_attrNames_head) if i not in indexes]\n",
    "    df_full = df_full.drop(columns=en_attrNames_head)\n",
    "\n",
    "    # Step 7: We join the two tables with different granularity\n",
    "    df_joined = pd.merge(df, df_full, on=['latitude', 'longitude', 'date', 'hour'])\n",
    "    arr_df_full.append(df_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e2e1fb",
   "metadata": {},
   "source": [
    "`arr_df_full` is an array of DataFrames. In each DataFrame one instance is a person or a withness involved in an accidents, but also has the details about the accident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3df734",
   "metadata": {},
   "source": [
    "Now, let's produce the file `INTEGRATION_brasilEnglishFull.csv`, which is the result of the join operation we've performed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d435976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "out_dir = editedDataset_folder\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "file_path = os.path.join(out_dir, 'INTEGRATION_brasilEnglishFull.csv')\n",
    "if not os.path.exists(file_path):\n",
    "    df_all = pd.concat(arr_df_full, ignore_index=True)\n",
    "    df_all.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a9d85",
   "metadata": {},
   "source": [
    "### Road feature integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd0fbf",
   "metadata": {},
   "source": [
    "Now, we'll exploit Overpass API to gather some additional details about the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1aa491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are specifying the range for the indexes of the DataFrame in which\n",
    "# we are interested on to look for the speedlimits.\n",
    "start_indexes = [400,100000]\n",
    "end_indexes = [500,100100]\n",
    "\n",
    "integrate_road = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e27fd232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention: takes a lot of time!\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from json import JSONDecodeError\n",
    "from time import sleep\n",
    "\n",
    "url = \"http://overpass-api.de/api/interpreter\"\n",
    "overpass_dfs = []\n",
    "\n",
    "if integrate_road:\n",
    "    \n",
    "    for start_index, end_index in zip(start_indexes, end_indexes):\n",
    "        df_coords_unique = df.iloc[start_index:end_index+1].loc[:,'latitude':'longitude'].drop_duplicates()\n",
    "        df_coords_unique\n",
    "\n",
    "        tags = []\n",
    "        for index, row in df_coords_unique.iterrows():\n",
    "            latitude = row['latitude']\n",
    "            longitude = row['longitude']\n",
    "\n",
    "            query = f\"\"\"\n",
    "            [out:json][timeout:25];\n",
    "            way(around:8,{latitude},{longitude})[highway];\n",
    "            out tags;\n",
    "            \"\"\"\n",
    "            \n",
    "            # retry loop\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    response = requests.get(url, params={'data': query}, timeout=40)\n",
    "                    if response.status_code == 200:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Attempt {attempt+1}/{max_retries} failed with status {response.status_code}\")\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Tentativo {attempt+1}/{max_retries} fallito con errore: {e}\")\n",
    "                sleep(2)\n",
    "            else:\n",
    "                # se esco dal ciclo senza break, vuol dire che tutti i tentativi sono falliti\n",
    "                print(f\"All attempts failed at {index}, skipping...\")\n",
    "                continue\n",
    "            response = requests.get(url, params={'data': query})\n",
    "            \n",
    "            print(f\"Processing index: {index}\")\n",
    "            try:\n",
    "                data = response.json()\n",
    "                if data[\"elements\"]:\n",
    "                    elements_arr = data[\"elements\"]\n",
    "                    for elem in elements_arr:\n",
    "                        if elem[\"tags\"]:\n",
    "                            dict = elem[\"tags\"]\n",
    "                            # I want to keep only: maxspeed, lanes, name, operator, toll, surface, oneway\n",
    "                            # I use a list comprehesion\n",
    "                            keep = {\"maxspeed\", \"lanes\", \"name\", \"operator\", \"toll\", \"surface\", \"oneway\"}\n",
    "                            dict = {k: v for k, v in elem[\"tags\"].items() if k in keep}\n",
    "                            dict['latitude'] = latitude\n",
    "                            dict['longitude'] = longitude\n",
    "                            tags.append(dict)\n",
    "            except JSONDecodeError:\n",
    "                print(\"Returned JSON Decode Error.. Continuing..\")\n",
    "                continue\n",
    "\n",
    "        overpass_df = pd.DataFrame(tags)\n",
    "        overpass_dfs.append(overpass_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d54ae1",
   "metadata": {},
   "source": [
    "Output when `integrate_road = True`:\n",
    "\n",
    "```text\n",
    "Processing index: 24936\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 24916\n",
    "Processing index: 50\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 56\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24918\n",
    "Processing index: 24939\n",
    "Processing index: 25371\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 49\n",
    "Processing index: 3370\n",
    "Processing index: 43\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 24906\n",
    "Processing index: 24927\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 40777\n",
    "Processing index: 72800\n",
    "Processing index: 24919\n",
    "Processing index: 45\n",
    "Processing index: 48\n",
    "Processing index: 24910\n",
    "Processing index: 46\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24915\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24917\n",
    "Processing index: 48883\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 57\n",
    "Processing index: 24930\n",
    "Processing index: 47\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24946\n",
    "Processing index: 24929\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 24928\n",
    "Processing index: 24965\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24954\n",
    "Processing index: 24925\n",
    "Processing index: 54\n",
    "Processing index: 89360\n",
    "Processing index: 24922\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24920\n",
    "Processing index: 24935\n",
    "Processing index: 24923\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 24933\n",
    "Processing index: 53\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24926\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 504\n",
    "Attempt 2/3 failed with status 504\n",
    "Processing index: 24937\n",
    "Processing index: 51\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 24931\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 24969\n",
    "Processing index: 24924\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 31290\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 429\n",
    "Attempt 2/3 failed with status 504\n",
    "Attempt 3/3 failed with status 504\n",
    "All attempts failed at 25023, skipping...\n",
    "Processing index: 24932\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24941\n",
    "Processing index: 24934\n",
    "Attempt 1/3 failed with status 429\n",
    "Attempt 2/3 failed with status 504\n",
    "Processing index: 52\n",
    "Processing index: 24940\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24938\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 25263\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 55\n",
    "Processing index: 24948\n",
    "Processing index: 24962\n",
    "Processing index: 25000\n",
    "Processing index: 24527\n",
    "Processing index: 24993\n",
    "Processing index: 24942\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 24944\n",
    "Processing index: 24947\n",
    "Attempt 1/3 failed with status 504\n",
    "Attempt 2/3 failed with status 504\n",
    "Processing index: 70\n",
    "Processing index: 24945\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24998\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24960\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 65\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 24992\n",
    "Processing index: 24955\n",
    "Processing index: 58\n",
    "Processing index: 24967\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24964\n",
    "Processing index: 24968\n",
    "Processing index: 25009\n",
    "Processing index: 24975\n",
    "Processing index: 24949\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24951\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 24958\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 24970\n",
    "Processing index: 60\n",
    "Attempt 1/3 failed with status 429\n",
    "Attempt 2/3 failed with status 504\n",
    "Processing index: 24961\n",
    "Processing index: 24952\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 24956\n",
    "Processing index: 24977\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 24999\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24985\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 24953\n",
    "Processing index: 24957\n",
    "Processing index: 24959\n",
    "Processing index: 24972\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 61\n",
    "Processing index: 24966\n",
    "Processing index: 24986\n",
    "Processing index: 35394\n",
    "Processing index: 24982\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 24963\n",
    "Processing index: 24974\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 66656\n",
    "Processing index: 72\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 92195\n",
    "Processing index: 92196\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 93668\n",
    "Processing index: 92197\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 115032\n",
    "Processing index: 115038\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 115034\n",
    "Processing index: 92198\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 115033\n",
    "Processing index: 115051\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115041\n",
    "Processing index: 115060\n",
    "Attempt 1/3 failed with status 504\n",
    "Attempt 2/3 failed with status 504\n",
    "Processing index: 115043\n",
    "Processing index: 92200\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 92199\n",
    "Processing index: 115047\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 504\n",
    "Attempt 2/3 failed with status 504\n",
    "Processing index: 92201\n",
    "Processing index: 92224\n",
    "Attempt 1/3 failed with status 429\n",
    "Attempt 2/3 failed with status 429\n",
    "Processing index: 115039\n",
    "Processing index: 115045\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 115066\n",
    "Processing index: 115063\n",
    "Processing index: 115046\n",
    "Processing index: 115056\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115049\n",
    "Processing index: 115042\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 115040\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115044\n",
    "Processing index: 126237\n",
    "Processing index: 115048\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115092\n",
    "Processing index: 92205\n",
    "Processing index: 115062\n",
    "Processing index: 115050\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 115052\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 115053\n",
    "Processing index: 115054\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115057\n",
    "Processing index: 92219\n",
    "Processing index: 115055\n",
    "Processing index: 92202\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115058\n",
    "Processing index: 115073\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115059\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 115061\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115067\n",
    "Processing index: 115064\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 504\n",
    "Attempt 2/3 failed with status 504\n",
    "Processing index: 115065\n",
    "Processing index: 115104\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 92206\n",
    "Processing index: 92208\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 92228\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 92225\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115070\n",
    "Processing index: 115076\n",
    "Attempt 1/3 failed with status 429\n",
    "Attempt 2/3 failed with status 504\n",
    "Processing index: 115068\n",
    "Processing index: 92209\n",
    "Attempt 1/3 failed with status 429\n",
    "Attempt 2/3 failed with status 504\n",
    "Processing index: 115083\n",
    "Processing index: 115077\n",
    "Attempt 1/3 failed with status 429\n",
    "Attempt 2/3 failed with status 429\n",
    "Processing index: 115069\n",
    "Processing index: 115071\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 92212\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115120\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115072\n",
    "Processing index: 115075\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115080\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 115096\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 92213\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 92223\n",
    "Processing index: 115110\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115078\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 136457\n",
    "Processing index: 115106\n",
    "Processing index: 115082\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 115079\n",
    "Processing index: 118541\n",
    "Processing index: 115101\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115081\n",
    "Processing index: 115087\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115090\n",
    "Processing index: 115102\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 115084\n",
    "Processing index: 115085\n",
    "Processing index: 115091\n",
    "Processing index: 115094\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 115107\n",
    "Processing index: 115089\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115086\n",
    "Processing index: 107543\n",
    "Processing index: 115088\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115095\n",
    "Processing index: 107547\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 115093\n",
    "Processing index: 115100\n",
    "Attempt 1/3 failed with status 504\n",
    "Processing index: 92220\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 92217\n",
    "Processing index: 92214\n",
    "Attempt 1/3 failed with status 429\n",
    "Processing index: 119972\n",
    "Processing index: 115112\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "Processing index: 115108\n",
    "Returned JSON Decode Error.. Continuing..\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "192b10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if overpass_dfs:\n",
    "    overpass_dfs[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a6368",
   "metadata": {},
   "source": [
    "Output when `integrate_road = True`:\n",
    "\n",
    "```text\n",
    "74\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "223b75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if overpass_dfs:\n",
    "    overpass_dfs[1].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ac72e",
   "metadata": {},
   "source": [
    "Output when `integrate_road = True`:\n",
    "\n",
    "```text\n",
    "60\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d622d4",
   "metadata": {},
   "source": [
    "From the 100 instances we start from, only 60-70 satisfy the OverPass API query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4991b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overpass_dfs_clean = []\n",
    "for overpass_df in overpass_dfs:\n",
    "    overpass_df_clean = overpass_df.dropna(subset=['lanes', 'oneway', ])\n",
    "    overpass_dfs_clean.append(overpass_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0beb5878",
   "metadata": {},
   "outputs": [],
   "source": [
    "if overpass_dfs_clean:\n",
    "    overpass_dfs_clean[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1d07a",
   "metadata": {},
   "source": [
    "Output when `integrate_road = True`:\n",
    "```text\n",
    "34\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2968ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if overpass_dfs_clean:\n",
    "    overpass_dfs_clean[1].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82228de5",
   "metadata": {},
   "source": [
    "Output when `integrate_road = True`:\n",
    "```text\n",
    "35\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a9be8",
   "metadata": {},
   "source": [
    "Since in both cases we obtained that from 100 instances we begin from, only 25-30 of them have associated information concerning the speedlimit and other road features I decide to not consider this aspect in the project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMMLProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
